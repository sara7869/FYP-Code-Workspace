{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Install the Tensorflow library using pip as the Python package manager.\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user matplotlib==3.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# ChatGPT - DL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, ZeroPadding1D\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.layers import Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout, concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import itertools\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "\n",
    "# libraries for models\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "# Libraries for Metrics evaluation\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, roc_curve, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data = pd.read_csv(\"./dataset/Employee Analysis Attrition Report/HR Employee Attrition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analysis of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [column for column in project_data.columns if project_data[column].dtype == 'int64']\n",
    "print(numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in numeric_columns:\n",
    "    plt.figure(figsize=(3,2))\n",
    "    sns.kdeplot(data=project_data, x=column, palette=\"crest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [column for column in project_data.columns if project_data[column].dtype != 'int64']\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categorical_columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(x=project_data[column])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in numeric_columns:\n",
    "    plt.figure(figsize=(3,2))\n",
    "    sns.kdeplot(data=project_data, x=column, hue=\"Attrition\", fill=True, alpha=.5, palette=\"crest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = project_data.copy()\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df1[column] = encoder.fit_transform(df1[column])\n",
    "\n",
    "plt.figure(figsize=(30,12))\n",
    "corr = df1.corr()\n",
    "sns.heatmap(corr, annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=project_data.drop(columns=[\"Attrition\"])\n",
    "y_train=project_data[\"Attrition\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset shape:',X_train.shape)\n",
    "print('Test dataset shape', X_test.shape)\n",
    "\n",
    "print('Train dataset rows: ', len(X_train))\n",
    "print('Test dataset rows: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = X_train.select_dtypes(exclude='object').columns\n",
    "print(numeric_columns)\n",
    "print('*'*100)\n",
    "categorical_columns = X_train.select_dtypes(include='object').columns\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = Pipeline([\n",
    "    ('handlingmissingvalues',SimpleImputer(strategy='median')),\n",
    "    ('scaling',StandardScaler(with_mean=True))\n",
    "])\n",
    "\n",
    "print(numeric_features)\n",
    "print('*'*100)\n",
    "\n",
    "categorical_features = Pipeline([\n",
    "    ('handlingmissingvalues',SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoding', OneHotEncoder()),\n",
    "    ('scaling', StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "print(categorical_features)\n",
    "\n",
    "processing = ColumnTransformer([\n",
    "    ('numeric', numeric_features, numeric_columns),\n",
    "    ('categorical', categorical_features, categorical_columns)\n",
    "])\n",
    "\n",
    "processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Methods for Model Preparation & Metric Evaliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_for_ml(name, algorithm):\n",
    "    \n",
    "    # model = Pipeline(steps=[\n",
    "    #     ('processing', processing),  # Assuming you have defined preprocessing steps\n",
    "    #     ('pca', TruncatedSVD(n_components=3, random_state=12))\n",
    "    # ])\n",
    "\n",
    "    model = Pipeline(steps= [\n",
    "        ('processing',processing),\n",
    "        ('pca', TruncatedSVD(n_components=3, random_state=12)),\n",
    "        ('modeling', algorithm)\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save\n",
    "    filename = \"Employee Analysis Attrition Report - \"+ name +'.pkl'\n",
    "    print(filename)\n",
    "    # with open('C:/Users/saray/Documents/FYP Code Workspace/model/1. Employee Analysis Attrition Report', 'wb') as f:\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_for_dl(name, algorithm):\n",
    "    \n",
    "    model = Pipeline(steps=[\n",
    "        ('processing', processing),  # Assuming you have defined preprocessing steps\n",
    "        ('pca', TruncatedSVD(n_components=3, random_state=12))\n",
    "    ])\n",
    "    \n",
    "    # Save\n",
    "    filename = \"Employee Analysis Attrition Report - \"+ name +'.pkl'\n",
    "    print(filename)\n",
    "    # with open('C:/Users/saray/Documents/FYP Code Workspace/model/1. Employee Analysis Attrition Report', 'wb') as f:\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row 1100 - No\n",
    "input_data1 = {\n",
    "    \"Age\": [40],\n",
    "    \"BusinessTravel\": ['Non-Travel'],\n",
    "    \"DailyRate\": [1142],\n",
    "    \"Department\": ['Research & Development'],\n",
    "    \"DistanceFromHome\": [8],\n",
    "    \"Education\": [2],\n",
    "    \"EducationField\": ['Life Sciences'],\n",
    "    \"EmployeeCount\": [1],\n",
    "    \"EmployeeNumber\": [1552],\n",
    "    \"EnvironmentSatisfaction\": [4],\n",
    "    \"Gender\": ['Male'],\n",
    "    \"HourlyRate\": [72],\n",
    "    \"JobInvolvement\": [3],\n",
    "    \"JobLevel\": [2],\n",
    "    \"JobRole\": ['Healthcare Representative'],\n",
    "    \"JobSatisfaction\": [4],\n",
    "    \"MaritalStatus\": ['Divorced'],\n",
    "    \"MonthlyIncome\": [4069],\n",
    "    \"MonthlyRate\": [8841],\n",
    "    \"NumCompaniesWorked\": [3],\n",
    "    \"Over18\": ['Y'],\n",
    "    \"OverTime\": ['Yes'],\n",
    "    \"PercentSalaryHike\": [18],\n",
    "    \"PerformanceRating\": [3],\n",
    "    \"RelationshipSatisfaction\": [3],\n",
    "    \"StandardHours\": [80],\n",
    "    \"StockOptionLevel\": [0],\n",
    "    \"TotalWorkingYears\": [8],\n",
    "    \"TrainingTimesLastYear\": [2],\n",
    "    \"WorkLifeBalance\": [3],\n",
    "    \"YearsAtCompany\": [2],\n",
    "    \"YearsInCurrentRole\": [2],\n",
    "    \"YearsSinceLastPromotion\": [2],\n",
    "    \"YearsWithCurrManager\": [2]\n",
    "}\n",
    "\n",
    "# Row 999 - Yes\n",
    "input_data2 = {\n",
    "    \"Age\": [27],\n",
    "    \"BusinessTravel\": ['Travel_Rarely'],\n",
    "    \"DailyRate\": [135],\n",
    "    \"Department\": ['Research & Development'],\n",
    "    \"DistanceFromHome\": [17],\n",
    "    \"Education\": [4],\n",
    "    \"EducationField\": ['Life Sciences'],\n",
    "    \"EmployeeCount\": [1],\n",
    "    \"EmployeeNumber\": [1405],\n",
    "    \"EnvironmentSatisfaction\": [4],\n",
    "    \"Gender\": ['Female'],\n",
    "    \"HourlyRate\": [51],\n",
    "    \"JobInvolvement\": [3],\n",
    "    \"JobLevel\": [2],\n",
    "    \"JobRole\": ['Research Scientist'],\n",
    "    \"JobSatisfaction\": [3],\n",
    "    \"MaritalStatus\": ['Single'],\n",
    "    \"MonthlyIncome\": [2394],\n",
    "    \"MonthlyRate\": [25681],\n",
    "    \"NumCompaniesWorked\": [1],\n",
    "    \"Over18\": ['Y'],\n",
    "    \"OverTime\": ['Yes'],\n",
    "    \"PercentSalaryHike\": [13],\n",
    "    \"PerformanceRating\": [3],\n",
    "    \"RelationshipSatisfaction\": [4],\n",
    "    \"StandardHours\": [80],\n",
    "    \"StockOptionLevel\": [0],\n",
    "    \"TotalWorkingYears\": [8],\n",
    "    \"TrainingTimesLastYear\": [2],\n",
    "    \"WorkLifeBalance\": [3],\n",
    "    \"YearsAtCompany\": [8],\n",
    "    \"YearsInCurrentRole\": [2],\n",
    "    \"YearsSinceLastPromotion\": [7],\n",
    "    \"YearsWithCurrManager\": [7]\n",
    "}\n",
    "\n",
    "# Row 23 - Yes\n",
    "input_data3 = {\n",
    "    \"Age\": [36],\n",
    "    \"BusinessTravel\": ['Travel_Rarely'],\n",
    "    \"DailyRate\": [1218],\n",
    "    \"Department\": ['Sales'],\n",
    "    \"DistanceFromHome\": [9],\n",
    "    \"Education\": [4],\n",
    "    \"EducationField\": ['Life Sciences'],\n",
    "    \"EmployeeCount\": [1],\n",
    "    \"EmployeeNumber\": [27],\n",
    "    \"EnvironmentSatisfaction\": [3],\n",
    "    \"Gender\": ['Male'],\n",
    "    \"HourlyRate\": [82],\n",
    "    \"JobInvolvement\": [2],\n",
    "    \"JobLevel\": [1],\n",
    "    \"JobRole\": ['Sales Representative'],\n",
    "    \"JobSatisfaction\": [1],\n",
    "    \"MaritalStatus\": ['Single'],\n",
    "    \"MonthlyIncome\": [3407],\n",
    "    \"MonthlyRate\": [6986],\n",
    "    \"NumCompaniesWorked\": [7],\n",
    "    \"Over18\": ['Y'],\n",
    "    \"OverTime\": ['No'],\n",
    "    \"PercentSalaryHike\": [23],\n",
    "    \"PerformanceRating\": [4],\n",
    "    \"RelationshipSatisfaction\": [2],\n",
    "    \"StandardHours\": [80],\n",
    "    \"StockOptionLevel\": [0],\n",
    "    \"TotalWorkingYears\": [10],\n",
    "    \"TrainingTimesLastYear\": [4],\n",
    "    \"WorkLifeBalance\": [3],\n",
    "    \"YearsAtCompany\": [5],\n",
    "    \"YearsInCurrentRole\": [3],\n",
    "    \"YearsSinceLastPromotion\": [0],\n",
    "    \"YearsWithCurrManager\": [3]\n",
    "}\n",
    "\n",
    "# Row 53 - Yes\n",
    "input_data4 = {\n",
    "    \"Age\": [28],\n",
    "    \"BusinessTravel\": ['Travel_Rarely'],\n",
    "    \"DailyRate\": [1434],\n",
    "    \"Department\": ['Research & Development'],\n",
    "    \"DistanceFromHome\": [5],\n",
    "    \"Education\": [4],\n",
    "    \"EducationField\": ['Technical Degree'],\n",
    "    \"EmployeeCount\": [1],\n",
    "    \"EmployeeNumber\": [65],\n",
    "    \"EnvironmentSatisfaction\": [3],\n",
    "    \"Gender\": ['Male'],\n",
    "    \"HourlyRate\": [50],\n",
    "    \"JobInvolvement\": [3],\n",
    "    \"JobLevel\": [1],\n",
    "    \"JobRole\": ['Laboratory Technician'],\n",
    "    \"JobSatisfaction\": [3],\n",
    "    \"MaritalStatus\": ['Single'],\n",
    "    \"MonthlyIncome\": [3441],\n",
    "    \"MonthlyRate\": [11179],\n",
    "    \"NumCompaniesWorked\": [1],\n",
    "    \"Over18\": ['Y'],\n",
    "    \"OverTime\": ['Yes'],\n",
    "    \"PercentSalaryHike\": [13],\n",
    "    \"PerformanceRating\": [3],\n",
    "    \"RelationshipSatisfaction\": [3],\n",
    "    \"StandardHours\": [80],\n",
    "    \"StockOptionLevel\": [0],\n",
    "    \"TotalWorkingYears\": [2],\n",
    "    \"TrainingTimesLastYear\": [3],\n",
    "    \"WorkLifeBalance\": [2],\n",
    "    \"YearsAtCompany\": [2],\n",
    "    \"YearsInCurrentRole\": [2],\n",
    "    \"YearsSinceLastPromotion\": [2],\n",
    "    \"YearsWithCurrManager\": [2]\n",
    "}\n",
    "\n",
    "input_data_df1 = pd.DataFrame(input_data1)\n",
    "input_data_df2 = pd.DataFrame(input_data2)\n",
    "input_data_df3 = pd.DataFrame(input_data3)\n",
    "input_data_df4 = pd.DataFrame(input_data4)\n",
    "\n",
    "input_data = [input_data_df1, input_data_df2, input_data_df3, input_data_df4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_confusion_matrix(cm, classes, model_name, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     fmt = 'd'\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i in range(cm.shape[0]):\n",
    "#         for j in range(cm.shape[1]):\n",
    "#             plt.text(j, i, format(cm[i, j], fmt),\n",
    "#                      ha=\"center\", va=\"center\",\n",
    "#                      color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "\n",
    "#     print(model_name)\n",
    "#     plt.show()\n",
    "\n",
    "# Define a function to plot the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, model_name):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_confusion_matrix(algo, models, X_test, y_test):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.title(f'Confusion Matrix for {algo}')\n",
    "    print('--------------------------------------------------------------------------------')\n",
    "\n",
    "    for model in models:\n",
    "        print(\"model: \", model)\n",
    "        # model.summary()\n",
    "        if model.name.startswith('sequential_'):\n",
    "            y_pred = model.predict(X_test)  # Default for single-input models\n",
    "        elif model.name.startswith('model_'):\n",
    "            y_pred = model.predict([X_test, X_test])\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)  # Default for single-input models\n",
    "\n",
    "        # Adjust the threshold and create the confusion matrix\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "        cm = confusion_matrix(y_test, y_pred_binary)\n",
    "        plot_confusion_matrix(cm, classes=['Negative', 'Positive'], model_name=f'{algo}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_evaluation(model, model_name, X_test, y_test):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Initialize lists to store evaluation metrics\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    support_scores = []\n",
    "    specificity_scores = []\n",
    "    misclassification_rates = []\n",
    "\n",
    "        \n",
    "    # Make predictions\n",
    "    if model.name.startswith('sequential'):\n",
    "        y_pred = model.predict(X_test)  # Default for single-input models\n",
    "        print('model starts with sequential')\n",
    "    elif model.name.startswith('model'):\n",
    "        y_pred = model.predict([X_test, X_test])\n",
    "    # else:\n",
    "    #     print(\"model name checked 3\")\n",
    "    #     y_pred = model.predict(X_test)  # Default for single-input models\n",
    "\n",
    "    # Adjust the threshold and create the confusion matrix\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_test, y_pred_binary)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred_binary))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "    precision_scores.append(precision_score(y_test, y_pred_binary))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_binary))\n",
    "    misclassification_rate = 1 - accuracy_score(y_test, y_pred_binary)\n",
    "    misclassification_rates.append(misclassification_rate)\n",
    "    \n",
    "    # Calculate support and specificity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    support_scores.append(tp + fn)\n",
    "    specificity_scores.append(tn / (tn + fp))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    # plt.subplot(1, len(models), i)\n",
    "    plot_confusion_matrix(cm, classes=['Negative', 'Positive'], model_name=model_name)\n",
    "\n",
    "    # Store metrics in dictionary\n",
    "    ensemble_metrics[model_name] = {\n",
    "        'Accuracy': np.mean(accuracy_scores),\n",
    "        'F1 Score': np.mean(f1_scores),\n",
    "        'Precision': np.mean(precision_scores),\n",
    "        'Recall': np.mean(recall_scores),\n",
    "        'Support': np.mean(support_scores),\n",
    "        'Specificity': np.mean(specificity_scores),\n",
    "        'Misclassification Rate': np.mean(misclassification_rates)\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_classification_report(algo, model):\n",
    "#     print(algo+' Report :')\n",
    "# #     pred = model.predict(X_test)\n",
    "# #     print(type(input_data_df))\n",
    "#     pred_testrow1 = model.predict(input_data_df1)\n",
    "#     pred_testrow2 = model.predict(input_data_df2)\n",
    "#     pred_testrow3 = model.predict(input_data_df3)\n",
    "#     pred_testrow4 = model.predict(input_data_df4)\n",
    "\n",
    "# #     print(classification_report(y_test, pred))\n",
    "#     print(\"My predictions: \", pred_testrow1,\", \", pred_testrow2,\", \", pred_testrow3,\", \", pred_testrow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [('bagging classifier', BaggingClassifier()), \n",
    "              ('KNN classifier', KNeighborsClassifier()), \n",
    "              ('Random Forest calssifier', RandomForestClassifier()), \n",
    "              ('Adaboost classifier', AdaBoostClassifier()), \n",
    "              ('Gradientboost classifier', GradientBoostingClassifier()),\n",
    "              ('MLP', MLPClassifier())\n",
    "             ]\n",
    "\n",
    "trained_models = []\n",
    "model_and_score = {}\n",
    "\n",
    "for index, tup in enumerate(algorithms):\n",
    "    model = prepare_model_for_ml(tup[0],tup[1])\n",
    "#     model_and_score[tup[0]] = str(model.score(X_train,y_train)*100)+\"%\"\n",
    "#     trained_models.append((tup[0],model))\n",
    "\n",
    "    # model = Pipeline(steps=[\n",
    "    #     ('processing', processing),  # Assuming you have defined preprocessing steps\n",
    "    #     ('pca', TruncatedSVD(n_components=3, random_state=12))\n",
    "    # ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_roc_curve(model, model_name, X_test, y_test):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "        \n",
    "    print(\"Model: \", model_name)\n",
    "    print()\n",
    "    # model.summary()\n",
    "    if model.name.startswith('sequential'):\n",
    "        y_score = model.predict(X_test)  # Default for single-input models\n",
    "    elif model.name.startswith('model'):\n",
    "        y_score = model.predict([X_test, X_test])\n",
    "    else:\n",
    "        y_score = model.predict(X_test)  # Default for single-input models\n",
    "        \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='grey', label='Random Guess')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'AUC ROC Curve for {model_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes - uncommented these 2 lines\n",
    "# trained_models = []\n",
    "# model_and_score = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fnn(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=512, step=32),\n",
    "                    activation='relu', input_dim=input_dim))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(Dense(units=hp.Int('units_2', min_value=32, max_value=256, step=32),\n",
    "                    activation='relu'))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wide_and_deep_model(hp):\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    wide_inputs = Input(shape=(input_dim,))\n",
    "    deep_inputs = Input(shape=(input_dim,))\n",
    "\n",
    "    wide_layer = Dense(units=hp.Int('wide_units', min_value=32, max_value=256, step=32),\n",
    "                       activation='relu')(wide_inputs)\n",
    "\n",
    "    deep_layer = Dense(units=hp.Int('deep_units_1', min_value=32, max_value=256, step=32),\n",
    "                      activation='relu')(deep_inputs)\n",
    "    deep_layer = Dense(units=hp.Int('deep_units_2', min_value=16, max_value=128, step=16),\n",
    "                      activation='relu')(deep_layer)\n",
    "\n",
    "    merged_layer = concatenate([wide_layer, deep_layer])\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(merged_layer)\n",
    "\n",
    "    model = Model(inputs=[wide_inputs, deep_inputs], outputs=output)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps['processing'].fit(X_train)\n",
    "\n",
    "# Transform the training and testing data using the preprocessing steps\n",
    "X_train_transformed = model.named_steps['processing'].transform(X_train)\n",
    "X_test_transformed = model.named_steps['processing'].transform(X_test)\n",
    "\n",
    "model.named_steps['pca'].fit(X_train_transformed)\n",
    "\n",
    "# Perform dimensionality reduction using TruncatedSVD\n",
    "X_train_svd = model.named_steps['pca'].transform(X_train_transformed)\n",
    "X_test_svd = model.named_steps['pca'].transform(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and transform the target variable\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(hp):\n",
    "    input_shape = (X_train_reshaped.shape[1], X_train_reshaped.shape[2])\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=hp.Int('filters', min_value=16, max_value=64, step=16),\n",
    "                     kernel_size=hp.Int('kernel_size', min_value=2, max_value=5, step=1),\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = np.expand_dims(X_train_svd, axis=2)\n",
    "X_test_reshaped = np.expand_dims(X_test_svd, axis=2)\n",
    "\n",
    "# Train the CNN Model\n",
    "# cnn_model = train_cnn_model(X_train_reshaped, y_train_encoded, X_test_reshaped, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the input arrays\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "# Check the content of the input arrays\n",
    "print(\"Sample of X_train:\")\n",
    "print(X_train[:5])  # Print the first 5 rows of X_train to verify its content\n",
    "print(\"Sample of y_train:\")\n",
    "print(y_train[:5])  # Print the first 5 elements of y_train to verify its content\n",
    "print(\"Sample of X_test:\")\n",
    "print(X_test[:5])   # Print the first 5 rows of X_test to verify its content\n",
    "print(\"Sample of y_test:\")\n",
    "print(y_test[:5])   # Print the first 5 elements of y_test to verify its content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train_svd\n",
    "y_train = y_train_encoded\n",
    "X_test = X_test_svd\n",
    "y_test = y_test_encoded\n",
    "\n",
    "# Check the content of the input arrays\n",
    "print(\"Sample of X_train:\")\n",
    "print(X_train[:5])  # Print the first 5 rows of X_train to verify its content\n",
    "print(\"Sample of y_train:\")\n",
    "print(y_train[:5])  # Print the first 5 elements of y_train to verify its content\n",
    "print(\"Sample of X_test:\")\n",
    "print(X_test[:5])   # Print the first 5 rows of X_test to verify its content\n",
    "print(\"Sample of y_test:\")\n",
    "print(y_test[:5])   # Print the first 5 elements of y_test to verify its content\n",
    "\n",
    "print(\"-------- FNN ----------\")\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "fnn_tuner = kt.RandomSearch(\n",
    "    create_fnn,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=50,\n",
    "    directory='my_dir',\n",
    "    project_name='fnn_hyperparameter_tuning')\n",
    "\n",
    "fnn_tuner.search(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
    "\n",
    "best_fnn_model = fnn_tuner.get_best_models(num_models=1)[0]\n",
    "best_fnn_hyperparameters = fnn_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "best_fnn_model.summary()\n",
    "\n",
    "best_fnn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "best_fnn_accuracy = best_fnn_model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(\"Best FNN Model Accuracy:\", best_fnn_accuracy)\n",
    "print(\"Best Hyperparameters:\", best_fnn_hyperparameters)\n",
    "\n",
    "print(\"-------- Wide and Deep ----------\")\n",
    "\n",
    "wd_tuner = RandomSearch(\n",
    "    create_wide_and_deep_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=50,\n",
    "    directory='my_dir',\n",
    "    project_name='wide_and_deep_hyperparameter_tuning'\n",
    ")\n",
    "\n",
    "wd_tuner.search_space_summary()\n",
    "wd_tuner.search([X_train, X_train], y_train, epochs=100, batch_size=32, validation_data=([X_test, X_test], y_test))\n",
    "\n",
    "best_wd_model = wd_tuner.get_best_models(num_models=1)[0]\n",
    "best_wd_hyperparameters = wd_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "best_wd_model.fit([X_train, X_train], y_train, epochs=100, batch_size=32, validation_data=([X_test, X_test], y_test))\n",
    "\n",
    "best_wide_and_deep_accuracy = best_wd_model.evaluate([X_test, X_test], y_test, verbose=0)[1]\n",
    "print(\"Best Wide & Deep Model Accuracy:\", best_wide_and_deep_accuracy)\n",
    "\n",
    "print(\"-------- CNN ----------\")\n",
    "\n",
    "X_train_reshaped = np.expand_dims(X_train_svd, axis=2)\n",
    "X_test_reshaped = np.expand_dims(X_test_svd, axis=2)\n",
    "\n",
    "print(\"X_train_reshaped shape:\", X_train_reshaped.shape)\n",
    "print(\"X_test_reshaped shape:\", X_test_reshaped.shape)\n",
    "\n",
    "cnn_tuner = kt.Hyperband(create_cnn_model,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=50,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "cnn_tuner.search(X_train_reshaped, y_train, epochs=100, validation_data=(X_test_reshaped, y_test))\n",
    "\n",
    "best_cnn_hyperparameters=cnn_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "best_cnn_model = cnn_tuner.hypermodel.build(best_cnn_hyperparameters)\n",
    "cnn_history = best_cnn_model.fit(X_train_reshaped, y_train, epochs=100, validation_data=(X_test_reshaped, y_test))\n",
    "\n",
    "best_cnn_accuracy = cnn_history.history['val_accuracy'][-1]\n",
    "print(\"Best CNN Model Accuracy:\", best_cnn_accuracy)\n",
    "\n",
    "models = [best_fnn_model, best_wd_model, best_cnn_model] \n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Accuracy for FNN:\", \"{:.2%}\".format(best_fnn_accuracy))\n",
    "print(\"Accuracy for Wide and Deep:\", \"{:.2%}\".format(best_wide_and_deep_accuracy))\n",
    "print(\"Accuracy for CNN:\", \"{:.2%}\".format(best_cnn_accuracy))\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the same dataset for training all models\n",
    "# X_train_all = X_train  # Assuming X_train is properly preprocessed\n",
    "\n",
    "# # Split the data consistently\n",
    "# X_train_fnn, X_train_wd, X_train_cnn = X_train, X_train, X_train_cnn\n",
    "\n",
    "# # Fit each model with its corresponding training data\n",
    "# best_fnn_model.fit(X_train_fnn, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "# best_wd_model.fit([X_train_wd, X_train_wd], y_train, epochs=100, batch_size=32, validation_data=([X_test, X_test], y_test))\n",
    "# best_cnn_model.fit(X_train_cnn, y_train, epochs=100, batch_size=32, validation_data=(X_test_cnn, y_test))\n",
    "\n",
    "# # Wrap each model with KerasClassifier/KerasRegressor\n",
    "# fnn_classifier = KerasClassifier(build_fn=lambda: best_fnn_model, epochs=100, batch_size=32, verbose=0)\n",
    "# wd_classifier = KerasClassifier(build_fn=lambda: best_wd_model, epochs=100, batch_size=32, verbose=0)\n",
    "# cnn_classifier = KerasClassifier(build_fn=lambda: best_cnn_model, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# # Define the voting classifier with the wrapped models\n",
    "# voting_clf = VotingClassifier(\n",
    "#     estimators=[('fnn', fnn_classifier), ('wd', wd_classifier), ('cnn', cnn_classifier)],\n",
    "#     voting='soft'  # Use soft voting for probabilities\n",
    "# )\n",
    "\n",
    "# # Fit the voting classifier\n",
    "# voting_clf.fit(X_train_all, y_train)\n",
    "\n",
    "# # Evaluate the voting classifier on the test set\n",
    "# voting_accuracy = voting_clf.score([X_test, X_test, X_test_cnn], y_test)\n",
    "# print(\"Voting Classifier Accuracy:\", voting_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_estimator_fnn = KerasClassifier(build_fn=create_fnn, hp=best_fnn_hyperparameters, epochs=100, batch_size=32, verbose=0)\n",
    "# keras_estimator_wd = KerasClassifier(build_fn=create_wide_and_deep_model, hp=best_wd_hyperparameters, epochs=100, batch_size=32, verbose=0)\n",
    "# keras_estimator_cnn = KerasClassifier(build_fn=create_cnn_model, hp=best_cnn_hyperparameters, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# # Define a list of (name, estimator) tuples for the VotingClassifier\n",
    "# estimators = [\n",
    "#     ('fnn', keras_estimator_fnn),\n",
    "#     # ('wide_and_deep', keras_estimator_wd),\n",
    "#     ('cnn', keras_estimator_cnn)\n",
    "# ]\n",
    "\n",
    "# keras_estimator_fnn.fit(X_train, y_train)\n",
    "# keras_estimator_cnn.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Shape of X_train:\", X_train.shape)\n",
    "# print(\"Shape of y_train:\", y_train.shape)\n",
    "\n",
    "# X_train_wd = [X_train, X_train]\n",
    "# print(\"Shape of X_train_wd[0]:\", X_train_wd[0].shape)\n",
    "# print(\"Shape of X_train_wd[1]:\", X_train_wd[1].shape)\n",
    "\n",
    "# # Fit the wide and deep model\n",
    "# # keras_estimator_wd.fit(X_train_wd, y_train)\n",
    "\n",
    "# # Define a list of (name, estimator) tuples for the VotingClassifier\n",
    "# estimators = [\n",
    "#     ('fnn', keras_estimator_fnn),\n",
    "#     # ('wide_and_deep', keras_estimator_wd),\n",
    "#     ('cnn', keras_estimator_cnn)\n",
    "# ]\n",
    "\n",
    "# voting_classifier = VotingClassifier(estimators=estimators, voting='hard')\n",
    "\n",
    "# # Fit the VotingClassifier on the training data\n",
    "# voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the ensemble classifier\n",
    "# ensemble_accuracy = voting_classifier.score(X_test, y_test)\n",
    "# print(\"Ensemble Accuracy:\", ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_train = y_train.replace({'Yes': 1, 'No': 0})\n",
    "# # y_test = np.where(y_test == 'Yes', 1, 0)\n",
    "# from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "\n",
    "# # Create a voting classifier with the trained models\n",
    "# voting_classifier = VotingClassifier(estimators=models, voting='hard')\n",
    "\n",
    "# # Fit the voting classifier on the training data\n",
    "# voting_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble_model(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    support = np.sum(y_true)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    misclassification_rate = 1 - accuracy\n",
    "    \n",
    "    print(f\"Evaluation Metrics for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Support: {support}\")\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    print(f\"Misclassification Rate: {misclassification_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_metamodel_fnn(input_dim):\n",
    "#     print(input_dim)\n",
    "#     model = Sequential([\n",
    "#         Dense(128, activation='relu', input_dim=input_dim),\n",
    "#         Dropout(0.2),\n",
    "#         Dense(64, activation='relu'),\n",
    "#         Dropout(0.2),\n",
    "#         Dense(1, activation='sigmoid')  # Use 'sigmoid' activation for binary classification\n",
    "#     ])\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "def create_metamodel_fnn(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=512, step=32),\n",
    "                    activation='relu',\n",
    "                    input_dim=input_dim))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(units=hp.Int('units_2', min_value=32, max_value=256, step=32),\n",
    "                    activation='relu'))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def tune_metamodel(X_train, y_train):\n",
    "    tuner = RandomSearch(\n",
    "        create_metamodel_fnn,\n",
    "        objective='val_accuracy',\n",
    "        max_trials=150,\n",
    "        executions_per_trial=3,\n",
    "        directory='stacking_hyperparameter_tuning',\n",
    "        project_name='stacking')\n",
    "\n",
    "    tuner.search(X_train, y_train, epochs=100, validation_split=0.2)\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    return best_hps\n",
    "\n",
    "def ensemble_predict(models, X_test, X_train, method):\n",
    "    test_predictions = []\n",
    "    train_predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        if isinstance(model, Sequential):\n",
    "            X_test_input = X_test \n",
    "            X_train_input = X_train\n",
    "        elif isinstance(model, Model):\n",
    "            X_test_input = (X_test, X_test)\n",
    "            X_train_input = (X_train, X_train)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model type provided.\")\n",
    "        \n",
    "        test_predictions.append(model.predict(X_test_input))\n",
    "        train_predictions.append(model.predict(X_train_input))\n",
    "    \n",
    "    if method == 'simple_average':\n",
    "        test_predictions = np.array(test_predictions)\n",
    "        ensemble_predictions = np.round(np.mean(test_predictions, axis=0)).astype(int)\n",
    "        evaluate_ensemble_model(y_test, ensemble_predictions, \"Simple Average\")\n",
    "        return ensemble_predictions\n",
    "    elif method == 'voting':\n",
    "        test_predictions = np.array(test_predictions)\n",
    "        num_samples = test_predictions.shape[1]\n",
    "        num_models = test_predictions.shape[0]\n",
    "        test_predictions = test_predictions.reshape(num_samples, num_models)\n",
    "        class_votes = np.sum(test_predictions, axis=1)\n",
    "        class_votes = np.expand_dims(class_votes, axis=1)\n",
    "        final_prediction = np.where(class_votes >= 0.5, 1, 0)\n",
    "        evaluate_ensemble_model(y_test, final_prediction, \"Voting\")\n",
    "        return final_prediction\n",
    "    elif method == 'stacking':\n",
    "        meta_X_train = np.concatenate(train_predictions, axis=1)\n",
    "        meta_X_test = np.concatenate(test_predictions, axis=1)\n",
    "\n",
    "        best_hps = tune_metamodel(meta_X_train, y_train)\n",
    "        print(best_hps)\n",
    "        print(\"Best Hyperparameters:\")\n",
    "        for param, value in best_hps.values.items():\n",
    "            print(f\"{param}: {value}\")\n",
    "            \n",
    "        meta_model = create_metamodel_fnn(best_hps)\n",
    "\n",
    "        meta_model.fit(meta_X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "        model_name = 'Stacking'\n",
    "        prepare_evaluation(meta_model, model_name, meta_X_test, y_test)\n",
    "        df_ensemble_metrics = pd.DataFrame(ensemble_metrics).T\n",
    "        print('df_ensemble_metrics')\n",
    "        print(df_ensemble_metrics)\n",
    "        print()\n",
    "        meta_model.save(\"stacking_ensemble_model.h5\")\n",
    "        meta_predictions = meta_model.predict(meta_X_test)\n",
    "        return np.round(meta_predictions).astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid ensemble method provided.\")\n",
    "\n",
    "stacking_accuracy = 0\n",
    "voting_accuracy = 0\n",
    "simple_average_accuracy = 0\n",
    "ensemble_accuracy = 0\n",
    "ensemble_metrics = {}\n",
    "meta_model = Sequential()\n",
    "meta_X_test = []\n",
    "\n",
    "ensemble_methods = ['stacking', 'voting', 'simple_average']\n",
    "\n",
    "for i, method in enumerate(ensemble_methods, 1):\n",
    "\n",
    "    print(f\"\\n-------- Ensemble Method: {method} ----------\")\n",
    "    ensemble_predictions = ensemble_predict(models, X_test, X_train, method)\n",
    "    ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "    ensemble_metrics[method] = ensemble_accuracy\n",
    "    print(f\"Accuracy of {method} method: {ensemble_accuracy}\")\n",
    "    if method == 'stacking':\n",
    "        stacking_accuracy = ensemble_accuracy\n",
    "    elif method == 'voting':\n",
    "        voting_accuracy = ensemble_accuracy\n",
    "    elif method == 'simple_average':\n",
    "        simple_average_accuracy = ensemble_accuracy\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(f\"Accuracy of {method} method: {ensemble_accuracy}\")\n",
    "    # print(f\"Predictions for {method} method:\", ensemble_predictions)  # Print predictions for each method\n",
    "    ensemble_accuracy = 0\n",
    "\n",
    "best_fnn_model.save(\"fnn_model.h5\")\n",
    "best_wd_model.save(\"wide_and_deep_model.h5\")\n",
    "best_cnn_model.save(\"cnn_model.h5\")\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Accuracy for Stacking:\", \"{:.2%}\".format(stacking_accuracy))\n",
    "print(\"Accuracy for Voting:\", \"{:.2%}\".format(voting_accuracy))\n",
    "print(\"Accuracy for Simple Average:\", \"{:.2%}\".format(simple_average_accuracy))\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_and_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models from pickle files\n",
    "dl_ensemble_models = []\n",
    "\n",
    "stacking_model = load_model(\"./stacking_ensemble_model.h5\")\n",
    "# simple_average_model = np.load(\"./simple_average_ensemble_model.npy\")\n",
    "# voting_model = np.load(\"./voting_ensemble_model.npy\")\n",
    "\n",
    "fnn_model = load_model(\"./fnn_model.h5\")\n",
    "wide_and_deep_model = load_model(\"./wide_and_deep_model.h5\")\n",
    "cnn_model = load_model(\"./cnn_model.h5\")\n",
    "\n",
    "# dl_base_models = [fnn_model, wide_and_deep_model, cnn_model]\n",
    "models = [best_fnn_model, best_wd_model, best_cnn_model] \n",
    "dl_base_models = models\n",
    "dl_ensemble_models = [meta_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_models\n",
    "\n",
    "dl_ensemble_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_base_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Confusion Matrix\n",
    "\n",
    "# Ensure y_test is encoded properly\n",
    "encoder = LabelEncoder()\n",
    "y_test_encoded = encoder.fit_transform(y_test)\n",
    "\n",
    "# Define a function to prepare confusion matrix and evaluation metrics for base models\n",
    "def prepare_evaluation(model, model_name, X_test, y_test):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    if model.name.startswith('sequential'):\n",
    "        y_pred_proba = model.predict(X_test)  # Default for single-input models\n",
    "    elif model.name.startswith('model'):\n",
    "        y_pred_proba = model.predict([X_test, X_test])\n",
    "\n",
    "    # Convert probabilities to binary predictions using a threshold of 0.5\n",
    "    y_pred_binary = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_binary)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    \n",
    "    # Calculate other metrics\n",
    "    f1 = f1_score(y_test, y_pred_binary)\n",
    "    precision = precision_score(y_test, y_pred_binary)\n",
    "    recall = recall_score(y_test, y_pred_binary)\n",
    "    misclassification_rate = 1 - accuracy\n",
    "    \n",
    "    # Calculate specificity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Update ensemble metrics dictionary\n",
    "    ensemble_metrics[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Support': tp + fn,\n",
    "        'Specificity': specificity,\n",
    "        'Misclassification Rate': misclassification_rate\n",
    "    }\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(cm, classes=['Negative', 'Positive'], model_name=model_name)\n",
    "\n",
    "\n",
    "df_base_model_metrics = []\n",
    "ensemble_metrics = {}\n",
    "\n",
    "# Call the function for base model evaluation\n",
    "for i, base_model in enumerate(dl_base_models, 1):\n",
    "    print(base_model)\n",
    "    if i == 1:\n",
    "        model_name = 'FNN'\n",
    "    elif i == 2:\n",
    "        model_name = 'Wide and Deep'\n",
    "    elif i == 3:\n",
    "        model_name = 'CNN'\n",
    "    prepare_evaluation(base_model, model_name, X_test_svd, y_test_encoded)\n",
    "\n",
    "# Print ensemble metrics as a table using pandas DataFrame\n",
    "df_base_model_metrics = pd.DataFrame(ensemble_metrics).T\n",
    "print()\n",
    "print(df_base_model_metrics)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, model_name):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# ensemble_metrics = {}  \n",
    "\n",
    "# print(dl_ensemble_models)\n",
    "# for i, ensemble_model in enumerate(dl_ensemble_models, 1):\n",
    "#     # print(ensemble_model)\n",
    "#     if i == 1:\n",
    "#         model_name = 'Stacking'\n",
    "#     elif i == 2:\n",
    "#         model_name = 'Voting'\n",
    "#     elif i == 3:\n",
    "#         model_name = 'Simple Average'\n",
    "#     print(model_name)\n",
    "#     prepare_evaluation(ensemble_model, model_name, meta_X_test, y_test)\n",
    "\n",
    "# df_ensemble_metrics = pd.DataFrame(ensemble_metrics).T\n",
    "# print()\n",
    "# print(df_ensemble_metrics)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for index, tup in enumerate(trained_models):\n",
    "#     prepare_classification_report(tup[0], tup[1])\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare ROC curve\n",
    "\n",
    "# Ensure y_test is encoded properly\n",
    "encoder = LabelEncoder()\n",
    "y_test_encoded = encoder.fit_transform(y_test)\n",
    "\n",
    "# Iterate over trained models and prepare ROC curves\n",
    "for i, model in enumerate(models, 1):\n",
    "    if i == 1:\n",
    "        model_name = 'FNN'\n",
    "    elif i == 2:\n",
    "        model_name = 'Wide and Deep'\n",
    "    elif i == 3:\n",
    "        model_name = 'CNN'\n",
    "    prepare_roc_curve(model, model_name, X_test_svd, y_test_encoded)\n",
    "\n",
    "\n",
    "# for i, model in enumerate(dl_ensemble_models, 1):\n",
    "#     print(model)\n",
    "#     if i == 1:\n",
    "#         model_name = 'Stacking'\n",
    "#     elif i == 2:\n",
    "#         model_name = 'Voting'\n",
    "#     elif i == 3:\n",
    "#         model_name = 'Simple Average'\n",
    "#     prepare_roc_curve(model, model_name, X_test_svd, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Classification Report\n",
    "\n",
    "# Transform input data using preprocessing steps\n",
    "input_data = [input_data_df1, input_data_df2, input_data_df3, input_data_df4]\n",
    "\n",
    "input_data_transformed = [model.named_steps['processing'].transform(data) for data in input_data]\n",
    "input_data_svd = [model.named_steps['pca'].transform(data_transformed) for data_transformed in input_data_transformed]\n",
    "\n",
    "# Define a function to convert probabilities to Yes/No labels\n",
    "def convert_to_yes_no(predictions):\n",
    "    return [\"Yes\" if pred > 0.5 else \"No\" for pred in predictions]\n",
    "\n",
    "# Make predictions using the deep learning ensemble models\n",
    "ensemble_predictions = {\n",
    "    'simple_avg': [],\n",
    "    'voting': [],\n",
    "    'stacking': []\n",
    "}\n",
    "\n",
    "# ensemble_predictions = {\n",
    "#     'simple_avg': [np.mean([model.predict(svd) for model in dl_ensemble_models_simple_avg], axis=0) for svd in input_data_svd],\n",
    "#     'voting': [np.mean([model.predict(svd) for model in dl_ensemble_models_voting], axis=0) for svd in input_data_svd],\n",
    "#     'stacking': [np.mean([model.predict(svd) for model in dl_ensemble_models_stacking], axis=0) for svd in input_data_svd]\n",
    "# }\n",
    "\n",
    "# for model in dl_ensemble_models_simple_avg:\n",
    "#     if isinstance(model, Sequential):  # Adjust 'CNNModel' to the actual class name of your CNN model\n",
    "#         input_data_reshaped = [np.expand_dims(svd, axis=2) for svd in input_data_svd]\n",
    "#         predictions = model.predict(input_data_reshaped)\n",
    "#     else:\n",
    "#         predictions = [model.predict(svd) for svd in input_data_svd]\n",
    "#     ensemble_predictions['simple_avg'].append(np.mean(predictions, axis=0))\n",
    "\n",
    "\n",
    "# for model in dl_ensemble_models_voting:\n",
    "#     if isinstance(model, Sequential):  # Adjust 'CNNModel' to the actual class name of your CNN model\n",
    "#         input_data_reshaped = [np.expand_dims(svd, axis=2) for svd in input_data_svd]\n",
    "#         predictions = model.predict(input_data_reshaped)\n",
    "#     else:\n",
    "#         predictions = [model.predict(svd) for svd in input_data_svd]\n",
    "#     ensemble_predictions['voting'].append(np.mean(predictions, axis=0))\n",
    "\n",
    "# for model in dl_ensemble_models_stacking:\n",
    "#     if isinstance(model, Sequential):  # Adjust 'CNNModel' to the actual class name of your CNN model\n",
    "#         input_data_reshaped = [np.expand_dims(svd, axis=2) for svd in input_data_svd]\n",
    "#         predictions = model.predict(input_data_reshaped)\n",
    "#     else:\n",
    "#         predictions = [model.predict(svd) for svd in input_data_svd]\n",
    "#     ensemble_predictions['stacking'].append(np.mean(predictions, axis=0))\n",
    "\n",
    "for model in dl_ensemble_models_simple_avg:\n",
    "    if isinstance(model, Sequential):\n",
    "        input_data_reshaped = [np.expand_dims(svd, axis=2) for svd in input_data_svd]\n",
    "        predictions = model.predict(input_data_reshaped)\n",
    "    else:\n",
    "        predictions = [model.predict(svd) for svd in input_data_svd]\n",
    "    ensemble_predictions['simple_avg'].append(predictions)\n",
    "\n",
    "for model in dl_ensemble_models_voting:\n",
    "    if isinstance(model, Sequential):\n",
    "        input_data_reshaped = [np.expand_dims(svd, axis=2) for svd in input_data_svd]\n",
    "        predictions = model.predict(input_data_reshaped)\n",
    "    else:\n",
    "        predictions = [model.predict(svd) for svd in input_data_svd]\n",
    "    ensemble_predictions['voting'].append(predictions)\n",
    "\n",
    "for model in dl_ensemble_models_stacking:\n",
    "    if isinstance(model, Sequential):\n",
    "        input_data_reshaped = [np.expand_dims(svd, axis=2) for svd in input_data_svd]\n",
    "        predictions = model.predict(input_data_reshaped)\n",
    "    else:\n",
    "        predictions = [model.predict(svd) for svd in input_data_svd]\n",
    "    ensemble_predictions['stacking'].append(predictions)\n",
    "\n",
    "# Convert ensemble predictions to Yes/No labels\n",
    "ensemble_labels = {\n",
    "    'simple_avg': [convert_to_yes_no(predictions) for predictions in ensemble_predictions['simple_avg']],\n",
    "    'voting': [convert_to_yes_no(predictions) for predictions in ensemble_predictions['voting']],\n",
    "    'stacking': [convert_to_yes_no(predictions) for predictions in ensemble_predictions['stacking']]\n",
    "}\n",
    "\n",
    "# Display the predictions\n",
    "for i, data in enumerate(input_data):\n",
    "    print(f\"\\nPredictions for input_data{i+1}:\")\n",
    "    print(\"Simple Average Ensemble:\", ensemble_labels['simple_avg'][i], \" Chance of Leaving: \", ensemble_predictions['simple_avg'][i])\n",
    "    print(\"Voting Ensemble:\", ensemble_labels['voting'][i], \" Chance of Leaving: \", ensemble_predictions['voting'][i])\n",
    "    print(\"Stacking Ensemble:\", ensemble_labels['stacking'][i], \" Chance of Leaving: \", ensemble_predictions['stacking'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare your input data (input_data_df1, input_data_df2, input_data_df3, input_data_df4)\n",
    "# input_data = [input_data_df1, input_data_df2, input_data_df3, input_data_df4]\n",
    "\n",
    "# # Preprocess the input data to match the expected input shapes of the models\n",
    "# # (You might need to adjust this based on your actual preprocessing steps)\n",
    "# input_data_transformed = [model.named_steps['processing'].transform(data) for data in input_data]\n",
    "# input_data_svd = [model.named_steps['pca'].transform(data_transformed) for data_transformed in input_data_transformed]\n",
    "\n",
    "# # Define a function to convert probabilities to Yes/No labels\n",
    "# def convert_to_yes_no(predictions):\n",
    "#     return [\"Yes\" if pred > 0.5 else \"No\" for pred in predictions]\n",
    "\n",
    "# # Make predictions using the deep learning ensemble models\n",
    "# ensemble_predictions = {\n",
    "#     'simple_avg': [],\n",
    "#     'voting': [],\n",
    "#     'stacking': []\n",
    "# }\n",
    "\n",
    "# # Iterate over each ensemble method\n",
    "# for ensemble_method in dl_ensemble_models:\n",
    "#     method_name = ensemble_method['method']\n",
    "#     models = ensemble_method['models']\n",
    "    \n",
    "#     # Iterate over each model in the ensemble\n",
    "#     for model in models:\n",
    "#         if isinstance(model, Sequential):  \n",
    "#             # Adjust input data shape for sequential models\n",
    "#             input_data_reshaped = [np.expand_dims(svd, axis=2) for svd in input_data_svd]\n",
    "#             predictions = model.predict(input_data_reshaped)\n",
    "#         else:\n",
    "#             predictions = [model.predict(svd) for svd in input_data_svd]\n",
    "        \n",
    "#         ensemble_predictions[method_name].append(predictions)\n",
    "\n",
    "# # Convert ensemble predictions to Yes/No labels\n",
    "# ensemble_labels = {\n",
    "#     'simple_avg': [convert_to_yes_no(predictions) for predictions in ensemble_predictions['simple_avg']],\n",
    "#     'voting': [convert_to_yes_no(predictions) for predictions in ensemble_predictions['voting']],\n",
    "#     'stacking': [convert_to_yes_no(predictions) for predictions in ensemble_predictions['stacking']]\n",
    "# }\n",
    "\n",
    "# # Display the predictions\n",
    "# for i, data in enumerate(input_data):\n",
    "#     print(f\"\\nPredictions for input_data{i+1}:\")\n",
    "#     print(\"Simple Average Ensemble:\", ensemble_labels['simple_avg'][i], \" Chance of Leaving: \", ensemble_predictions['simple_avg'][i])\n",
    "#     print(\"Voting Ensemble:\", ensemble_labels['voting'][i], \" Chance of Leaving: \", ensemble_predictions['voting'][i])\n",
    "#     print(\"Stacking Ensemble:\", ensemble_labels['stacking'][i], \" Chance of Leaving: \", ensemble_predictions['stacking'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2472819,
     "sourceId": 4192992,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30260,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
